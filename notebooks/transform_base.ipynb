{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257ce7e6-1188-46e2-ab32-7eb50dddc3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- configuracoes\n",
    "CONFIG_FILE = \"/Workspace/Users/joycelnog@gmail.com/pratica_ed/config.json\"\n",
    "cfg = json.loads(Path(CONFIG_FILE).read_text())\n",
    "BASE = cfg[\"base_path\"]\n",
    "SILVER_PATH  = f'{BASE}{cfg[\"paths\"][\"silver\"]}'        \n",
    "REJECT_PATH  = f'{BASE}rejects/silver_invalid/'           # pasta para inválidos\n",
    "\n",
    "spark.sql(\"USE CATALOG nyc_taxi\")\n",
    "spark.sql(\"USE SCHEMA data_area\")\n",
    "\n",
    "# --- limpar objetos antigos, caso existam fazer a carga completa\n",
    "spark.sql(\"DROP VIEW IF EXISTS nyc_taxi.data_area.v_silver_yellow_tripdata\")\n",
    "spark.sql(\"DROP VIEW IF EXISTS nyc_taxi.data_area.v_silver_invalid\")\n",
    "dbutils.fs.rm(SILVER_PATH, recurse=True)\n",
    "dbutils.fs.rm(REJECT_PATH, recurse=True)\n",
    "\n",
    "# --- base bronze\n",
    "bronze = spark.table(\"nyc_taxi.data_area.v_bronze_yellow_tripdata\")\n",
    "\n",
    "# --- normalizações \n",
    "s0 = (\n",
    "    bronze\n",
    "    .withColumn(\"ratecodeid\",      F.coalesce(F.col(\"ratecodeid\"), F.lit(99)))\n",
    "    .withColumn(\"store_and_fwd_flag\",\n",
    "                F.when(F.col(\"store_and_fwd_flag\").isNull() | (F.trim(F.col(\"store_and_fwd_flag\")) == \"\"), \"N\")\n",
    "                 .otherwise(F.upper(F.col(\"store_and_fwd_flag\"))))\n",
    "    .withColumn(\"store_and_fwd_flag\",\n",
    "                F.when(F.col(\"store_and_fwd_flag\").isin(\"Y\",\"N\"), F.col(\"store_and_fwd_flag\")).otherwise(F.lit(\"N\")))\n",
    "    .withColumn(\"passenger_count\", F.coalesce(F.col(\"passenger_count\"), F.lit(1)))\n",
    "    .withColumn(\"vendorid\",     F.col(\"vendorid\").cast(\"int\"))\n",
    "    .withColumn(\"ratecodeid\",   F.col(\"ratecodeid\").cast(\"int\"))\n",
    "    .withColumn(\"payment_type\", F.col(\"payment_type\").cast(\"int\"))\n",
    "    .withColumn(\"pulocationid\", F.col(\"pulocationid\").cast(\"int\"))\n",
    "    .withColumn(\"dolocationid\", F.col(\"dolocationid\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# --- verificacao de qualidade \n",
    "# --- 1) avalia os pickup e dropfoff, distancia e locations, se estão de acordo e consistente\n",
    "# --- 2) avalia datas pickups fora da data esperada no dataset de origem\n",
    "cond_pickup_before_dropoff = F.col(\"tpep_pickup_datetime\") < F.col(\"tpep_dropoff_datetime\")\n",
    "cond_trip_distance_non_neg = F.col(\"trip_distance\") >= 0\n",
    "cond_invalid_zero_distance = (\n",
    "    (F.col(\"trip_distance\") == 0) &\n",
    "    ~((F.col(\"tpep_pickup_datetime\") == F.col(\"tpep_dropoff_datetime\")) &\n",
    "      (F.col(\"pulocationid\") == F.col(\"dolocationid\")))\n",
    ")\n",
    "flag_outside_dataset = ((F.col(\"pickup_year\") != F.col(\"dataset_year\")) |\n",
    "                        (F.col(\"pickup_month\") != F.col(\"dataset_month\"))).cast(\"int\")\n",
    "\n",
    "s1 = s0.withColumn(\"is_pickup_outside_dataset\", flag_outside_dataset)\n",
    "\n",
    "# ---- separar registros que vou considerar como inválidos e válidos\n",
    "invalid = s1.where(\n",
    "    (~cond_pickup_before_dropoff) |\n",
    "    (F.col(\"trip_distance\") < 0) |\n",
    "    cond_invalid_zero_distance |\n",
    "    (F.col(\"is_pickup_outside_dataset\") == 1)\n",
    ")\n",
    "\n",
    "silver = s1.where(\n",
    "    cond_pickup_before_dropoff &\n",
    "    cond_trip_distance_non_neg &\n",
    "    ~cond_invalid_zero_distance &\n",
    "    (F.col(\"is_pickup_outside_dataset\") == 0)\n",
    ")\n",
    "\n",
    "# --- adiciono as descrições legíveis\n",
    "vendor_map = F.create_map(\n",
    "    [F.lit(1), F.lit(\"Creative Mobile Technologies\"),\n",
    "     F.lit(2), F.lit(\"Curb Mobility\"),\n",
    "     F.lit(6), F.lit(\"Myle Technologies\"),\n",
    "     F.lit(7), F.lit(\"Helix\")]\n",
    ")\n",
    "ratecode_map = F.create_map(\n",
    "    [F.lit(1), F.lit(\"Standard rate\"),\n",
    "     F.lit(2), F.lit(\"JFK\"),\n",
    "     F.lit(3), F.lit(\"Newark\"),\n",
    "     F.lit(4), F.lit(\"Nassau/Westchester\"),\n",
    "     F.lit(5), F.lit(\"Negotiated fare\"),\n",
    "     F.lit(6), F.lit(\"Group ride\"),\n",
    "     F.lit(99),F.lit(\"Null/Unknown\")]\n",
    ")\n",
    "payment_map = F.create_map(\n",
    "    [F.lit(0), F.lit(\"Flex Fare\"),\n",
    "     F.lit(1), F.lit(\"Credit card\"),\n",
    "     F.lit(2), F.lit(\"Cash\"),\n",
    "     F.lit(3), F.lit(\"No charge\"),\n",
    "     F.lit(4), F.lit(\"Dispute\"),\n",
    "     F.lit(5), F.lit(\"Unknown\"),\n",
    "     F.lit(6), F.lit(\"Voided trip\")]\n",
    ")\n",
    "\n",
    "silver = (silver\n",
    "          .withColumn(\"vendor_desc\",   F.coalesce(vendor_map[F.col(\"vendorid\")],   F.lit(\"Other/Unknown\")))\n",
    "          .withColumn(\"ratecode_desc\", F.coalesce(ratecode_map[F.col(\"ratecodeid\")], F.lit(\"Other/Unknown\")))\n",
    "          .withColumn(\"payment_desc\",  F.coalesce(payment_map[F.col(\"payment_type\")], F.lit(\"Other/Unknown\")))\n",
    ")\n",
    "\n",
    "# --- carrego os dados validos na delta principal e os invalidos para verificação posterior em outro lugar\n",
    "(silver.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .partitionBy(\"dataset_year\",\"dataset_month\")\n",
    "   .save(SILVER_PATH))\n",
    "\n",
    "(invalid.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .partitionBy(\"dataset_year\",\"dataset_month\")\n",
    "   .save(REJECT_PATH))\n",
    "\n",
    "# --- persistir as views apontando para os caminhos do delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW nyc_taxi.data_area.v_silver_yellow_tripdata AS\n",
    "SELECT * FROM delta.`{SILVER_PATH}`\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW nyc_taxi.data_area.v_silver_invalid AS\n",
    "SELECT * FROM delta.`{REJECT_PATH}`\n",
    "\"\"\")\n",
    "\n",
    "# --- checks da carga\n",
    "spark.sql(f\"\"\"\n",
    "SELECT dataset_year, dataset_month, COUNT(*) AS rows\n",
    "FROM nyc_taxi.data_area.v_silver_yellow_tripdata\n",
    "GROUP BY 1,2 ORDER BY 1,2\n",
    "\"\"\").display()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT dataset_year, dataset_month, COUNT(*) AS rows\n",
    "FROM nyc_taxi.data_area.v_silver_invalid\n",
    "GROUP BY 1,2 ORDER BY 1,2\n",
    "\"\"\").display()\n",
    "\n",
    "print(\"Silver carregada.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_base",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
