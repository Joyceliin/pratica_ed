{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5429915-3c90-4c1a-a671-79c05e5f76dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F\n",
    " \n",
    "# --- configurações ---\n",
    "CONFIG_FILE = \"/Workspace/Users/joycelnog@gmail.com/pratica_ed/config.json\"\n",
    "cfg = json.loads(Path(CONFIG_FILE).read_text())\n",
    "\n",
    "BASE = cfg[\"base_path\"]\n",
    "RAW_PATH = f'{BASE}{cfg[\"paths\"][\"raw\"]}'\n",
    "BRONZE_PATH = f'{BASE}{cfg[\"paths\"][\"bronze\"]}'\n",
    "\n",
    "spark.sql(\"USE CATALOG nyc_taxi\")\n",
    "spark.sql(\"USE SCHEMA data_area\")\n",
    "\n",
    "\n",
    "mapped_cols = {\n",
    "    \"vendorid\": \"int\",\n",
    "    \"tpep_pickup_datetime\": \"timestamp\",\n",
    "    \"tpep_dropoff_datetime\": \"timestamp\",\n",
    "    \"passenger_count\": \"int\",\n",
    "    \"trip_distance\": \"double\",\n",
    "    \"ratecodeid\": \"int\",\n",
    "    \"store_and_fwd_flag\": \"string\",\n",
    "    \"pulocationid\": \"int\",\n",
    "    \"dolocationid\": \"int\",\n",
    "    \"payment_type\": \"int\",\n",
    "    \"fare_amount\": \"double\",\n",
    "    \"extra\": \"double\",\n",
    "    \"mta_tax\": \"double\",\n",
    "    \"tip_amount\": \"double\",\n",
    "    \"tolls_amount\": \"double\",\n",
    "    \"improvement_surcharge\": \"double\",\n",
    "    \"total_amount\": \"double\",\n",
    "    \"congestion_surcharge\": \"double\",\n",
    "    \"airport_fee\": \"double\"\n",
    "}\n",
    "\n",
    "# -- tratamento para snake case, deixando as bases que serão apendadas terem os mesmos padrões de nome\n",
    "def to_snake(c: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9]\", \"_\", c).lower()\n",
    "\n",
    "# -- verifica se existe arquivo para subir\n",
    "files = sorted([f.path for f in dbutils.fs.ls(RAW_PATH) if f.path.endswith(\".parquet\")])\n",
    "assert files, f\"Nenhum arquivo parquet em {RAW_PATH}\"\n",
    "\n",
    "df_union = None\n",
    "for p in files:\n",
    "    # --- pega ano/mês do dataset a partir do nome do arquivo no padrão 2023 01\n",
    "    m = re.search(r\"(\\d{4})-(\\d{2})\\.parquet$\", p)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Não consegui extrair ano/mês do caminho: {p}\")\n",
    "    ds_year = int(m.group(1))\n",
    "    ds_month = int(m.group(2))\n",
    "\n",
    "    dfi = spark.read.parquet(p)\n",
    "    dfi = dfi.toDF(*[to_snake(c) for c in dfi.columns])\n",
    "\n",
    "    # --- normaliza 'airport_fee' (variações de capitalização)\n",
    "    for c in list(dfi.columns):\n",
    "        if c.lower() == \"airport_fee\" and c != \"airport_fee\":\n",
    "            dfi = dfi.withColumnRenamed(c, \"airport_fee\")\n",
    "\n",
    "    # --- verifica campos mapeados, não existindo, cria com valor nulo\n",
    "    for col, dtype in mapped_cols.items():\n",
    "        if col not in dfi.columns:\n",
    "            dfi = dfi.withColumn(col, F.lit(None).cast(dtype))\n",
    "        else:\n",
    "            dfi = dfi.withColumn(col, F.col(col).cast(dtype))\n",
    "\n",
    "    # --- adiciona colunas de partição da **origem** do dataset de acordo com o year e month do parquet\n",
    "    dfi = (dfi\n",
    "           .select(*mapped_cols.keys())\n",
    "           .withColumn(\"dataset_year\", F.lit(ds_year).cast(\"int\"))\n",
    "           .withColumn(\"dataset_month\", F.lit(ds_month).cast(\"int\")))\n",
    "\n",
    "    # --- mantém ano/mês do pickup para análise\n",
    "    dfi = (dfi\n",
    "           .withColumn(\"pickup_year\",  F.year(\"tpep_pickup_datetime\"))\n",
    "           .withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\")))\n",
    "\n",
    "    df_union = dfi if df_union is None else df_union.unionByName(dfi, allowMissingColumns=True)\n",
    "\n",
    "df_final = df_union\n",
    "\n",
    "# --- grava particionando pelos dados da **origem** (estável)\n",
    "(df_final.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"mergeSchema\", \"true\")   \n",
    "   .partitionBy(\"dataset_year\",\"dataset_month\")\n",
    "   .save(BRONZE_PATH))\n",
    "\n",
    "# --- persisto a view no catalogo apontando para o delta \n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW nyc_taxi.data_area.v_bronze_yellow_tripdata AS\n",
    "SELECT * FROM delta.`{BRONZE_PATH}`\n",
    "\"\"\")\n",
    "\n",
    "# --- checks da carga\n",
    "spark.sql(f\"\"\"\n",
    "SELECT dataset_year, dataset_month,\n",
    "       pickup_year, pickup_month,\n",
    "       COUNT(*) AS rows\n",
    "FROM nyc_taxi.data_area.v_bronze_yellow_tripdata\n",
    "GROUP BY 1,2,3,4\n",
    "ORDER BY 1,2,3,4\n",
    "\"\"\").display()\n",
    "\n",
    "print(\"Bronze atualizada\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_parquet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
