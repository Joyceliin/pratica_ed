{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "164ceff2-a4bc-4ad8-89fa-7b496ea26b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import argparse, json, runpy, sys, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "CATALOG = \"nyc_taxi\"\n",
    "SCHEMA  = \"data_area\"\n",
    "\n",
    "def spark():\n",
    "    return SparkSession.builder.getOrCreate()\n",
    "\n",
    "def read_cfg(p: str|Path) -> dict:\n",
    "    p = Path(p)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"config.json não encontrado: {p}\")\n",
    "    return json.loads(p.read_text())\n",
    "\n",
    "def list_parquets(prefix: str):\n",
    "    # exige Databricks (dbutils)\n",
    "    return [f for f in dbutils.fs.ls(prefix) if f.path.endswith(\".parquet\")]  # type: ignore\n",
    "\n",
    "def ensure_exec_logs():\n",
    "    s = spark()\n",
    "    s.sql(f\"USE CATALOG {CATALOG}\"); s.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "    s.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.pipeline_exec_log\n",
    "    (\n",
    "      stage        STRING,\n",
    "      started_at   TIMESTAMP,\n",
    "      finished_at  TIMESTAMP,\n",
    "      status       STRING,\n",
    "      message      STRING\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\")\n",
    "    s.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.ingestion_log\n",
    "    (\n",
    "      file_name     STRING,\n",
    "      file_path     STRING,\n",
    "      file_mod_time TIMESTAMP,\n",
    "      file_size     BIGINT,\n",
    "      status        STRING,\n",
    "      processed_at  TIMESTAMP\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\")\n",
    "\n",
    "def log_exec(stage: str, status: str, message: str = \"\"):\n",
    "    s = spark()\n",
    "    now = datetime.utcnow()\n",
    "    s.createDataFrame([(stage, now, now, status, message)],\n",
    "                      \"stage string, started_at timestamp, finished_at timestamp, status string, message string\"\n",
    "                      ).write.mode(\"append\").saveAsTable(f\"{CATALOG}.{SCHEMA}.pipeline_exec_log\")\n",
    "\n",
    "def detect_new_raw(cfg: dict) -> list[tuple]:\n",
    "    base = cfg[\"base_path\"]\n",
    "    raw  = f'{base}{cfg[\"paths\"][\"raw\"]}'\n",
    "    files = list_parquets(raw)\n",
    "    if not files: return []\n",
    "\n",
    "    s = spark()\n",
    "    s.sql(f\"USE CATALOG {CATALOG}\"); s.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "    df_log = s.table(f\"{CATALOG}.{SCHEMA}.ingestion_log\").select(\"file_name\").distinct()\n",
    "    already = set(r[\"file_name\"] for r in df_log.collect()) if df_log.count() > 0 else set()\n",
    "\n",
    "    new = []\n",
    "    for f in files:\n",
    "        name = Path(f.path).name\n",
    "        if name not in already:\n",
    "            # tenta extrair yyyy-mm do nome (ex.: yellow_tripdata_2023-04.parquet)\n",
    "            m = re.search(r\"(\\d{4})-(\\d{2})\\.parquet$\", name)\n",
    "            y, mth = (int(m.group(1)), int(m.group(2))) if m else (None, None)\n",
    "            new.append((name, f.path, f.modificationTime, f.size, y, mth))\n",
    "    return new\n",
    "\n",
    "def mark_ingested(rows: list[tuple], status: str, msg: str = \"\"):\n",
    "    if not rows: return\n",
    "    s = spark()\n",
    "    now = datetime.utcnow()\n",
    "    df = s.createDataFrame(\n",
    "        [(n, p, datetime.fromtimestamp(mt/1000.0), sz, status, now) for (n,p,mt,sz, *_rest) in rows],\n",
    "        \"file_name string, file_path string, file_mod_time timestamp, file_size long, status string, processed_at timestamp\"\n",
    "    )\n",
    "    df.write.mode(\"append\").saveAsTable(f\"{CATALOG}.{SCHEMA}.ingestion_log\")\n",
    "\n",
    "def run_stage_py(py_path: str, injected_globals: dict|None = None):\n",
    "    \"\"\"Executa um .py arbitrário no mesmo processo, injetando variáveis globais se necessário.\"\"\"\n",
    "    injected_globals = injected_globals or {}\n",
    "    # run_path executa o arquivo como se fosse __main__ mas aqui controlamos globals\n",
    "    runpy.run_path(py_path, init_globals=injected_globals)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Automation runner (bronze -> silver -> gold)\")\n",
    "    parser.add_argument(\"--config\", default=\"/Workspace/Users/joycelnog@gmail.com/pratica_ed/config.json\")\n",
    "    parser.add_argument(\"--stages\", default=\"bronze,silver,gold\",\n",
    "                        help=\"Lista de estágios separados por vírgula (bronze,silver,gold)\")\n",
    "    parser.add_argument(\"--only-if-new\", action=\"store_true\",\n",
    "                        help=\"Só roda Bronze se houver novos arquivos em RAW; se não, pula Bronze.\")\n",
    "    parser.add_argument(\"--overwrite\", default=\"true\", choices=[\"true\",\"false\"],\n",
    "                        help=\"Sinaliza aos scripts para sobrescrever (se suportado).\")\n",
    "    parser.add_argument(\"--scripts-root\", default=\"/Workspace/Users/joycelnog@gmail.com/pratica_ed/scripts\",\n",
    "                        help=\"Diretório onde estão bronze.py, silver.py, gold.py\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg = read_cfg(args.config)\n",
    "    ensure_exec_logs()\n",
    "\n",
    "    stages = [s.strip().lower() for s in args.stages.split(\",\") if s.strip()]\n",
    "    scripts_root = Path(args.scripts_root)\n",
    "\n",
    "    # caminhos dos seus scripts\n",
    "    bronze_py = str(scripts_root / \"bronze.py\")\n",
    "    silver_py = str(scripts_root / \"silver.py\")\n",
    "    gold_py   = str(scripts_root / \"gold.py\")\n",
    "\n",
    "    # variáveis que serão injetadas nos seus .py (se quiser ler)\n",
    "    injected = {\n",
    "        \"CONFIG_FILE\": args.config,\n",
    "        \"OVERWRITE\": args.overwrite.lower() == \"true\"\n",
    "    }\n",
    "\n",
    "    # --- BRONZE ---\n",
    "    if \"bronze\" in stages:\n",
    "        try:\n",
    "            new_files = detect_new_raw(cfg) if args.only-if-new else None\n",
    "        except Exception as e:\n",
    "            log_exec(\"bronze\", \"ERROR\", f\"detecção de novos arquivos falhou: {e}\")\n",
    "            raise\n",
    "\n",
    "        if args.only_if_new and new_files is not None and len(new_files) == 0:\n",
    "            log_exec(\"bronze\", \"SKIPPED\", \"sem novos arquivos em RAW\")\n",
    "        else:\n",
    "            try:\n",
    "                log_exec(\"bronze\", \"START\")\n",
    "                # Se quiser, injete a lista de novos arquivos para seu bronze.py usar:\n",
    "                if new_files is not None:\n",
    "                    injected[\"NEW_FILES\"] = [f for (_, f, *_rest) in new_files]\n",
    "                run_stage_py(bronze_py, injected)\n",
    "                # marca como sucesso no log de ingestão\n",
    "                if new_files:\n",
    "                    mark_ingested(new_files, \"SUCCESS\")\n",
    "                log_exec(\"bronze\", \"SUCCESS\")\n",
    "            except Exception as e:\n",
    "                # marca erro para os arquivos tentados\n",
    "                if new_files:\n",
    "                    mark_ingested(new_files, \"ERROR\", msg=str(e)[:500])\n",
    "                log_exec(\"bronze\", \"ERROR\", str(e)[:500])\n",
    "                raise\n",
    "\n",
    "    # --- SILVER ---\n",
    "    if \"silver\" in stages:\n",
    "        try:\n",
    "            log_exec(\"silver\", \"START\")\n",
    "            run_stage_py(silver_py, injected)\n",
    "            log_exec(\"silver\", \"SUCCESS\")\n",
    "        except Exception as e:\n",
    "            log_exec(\"silver\", \"ERROR\", str(e)[:500])\n",
    "            raise\n",
    "\n",
    "    # --- GOLD ---\n",
    "    if \"gold\" in stages:\n",
    "        try:\n",
    "            log_exec(\"gold\", \"START\")\n",
    "            run_stage_py(gold_py, injected)\n",
    "            log_exec(\"gold\", \"SUCCESS\")\n",
    "        except Exception as e:\n",
    "            log_exec(\"gold\", \"ERROR\", str(e)[:500])\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "automatizacao_data_area",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
